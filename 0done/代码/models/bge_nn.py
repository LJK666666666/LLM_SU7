"""
BGEç¥ç»ç½‘ç»œæ¨¡å‹ï¼šä½¿ç”¨BGE-base-zh-v1.5è¿›è¡Œæ–‡æœ¬ç¼–ç çš„æ·±åº¦å­¦ä¹ æ¨¡å‹

åŒ…å«:
- BERTç›¸å…³ç±»ï¼ˆEmbeddings, SelfAttention, Layer, Modelï¼‰
- CrossAttentionFusion: è·¨æ³¨æ„åŠ›èåˆå±‚
- DualPredictionHead: åŒé¢„æµ‹å¤´ï¼ˆå‡å€¼+æ–¹å·®ï¼‰
- CommentPredictorNN: è¯„è®ºé¢„æµ‹ç¥ç»ç½‘ç»œ
- BGENNModel: å°è£…ç±»
"""
import os
import re
import json
import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
from tqdm import tqdm

from ..config import (
    BGE_MODEL_PATH, LOG_OFFSET,
    VIP_USERS, VIP_USER_TO_ID, SPECIAL_TOKEN_USER,
    WEIBO_EMOJI_LIST, WEIBO_EMOJI_TO_ID,
    UNICODE_EMOJI_LIST, UNICODE_EMOJI_TO_ID,
    XIAOMI_EMBED_KEYWORDS, XIAOMI_KEYWORD_TO_ID,
    TOTAL_SPECIAL_EMBEDDINGS,
    VIP_EMBED_OFFSET, WEIBO_EMOJI_EMBED_OFFSET, UNICODE_EMOJI_EMBED_OFFSET,
    XIAOMI_EMBED_OFFSET, USER_EMBED_ID
)


# ==================== ç‰¹æ®ŠTokenæå– ====================
def extract_special_token_ids(text):
    """ä»æ–‡æœ¬ä¸­æå–ç‰¹æ®Štokençš„IDåˆ—è¡¨

    æå–å†…å®¹:
    1. @VIPç”¨æˆ· -> å¯¹åº”VIPåµŒå…¥ID
    2. @æ™®é€šç”¨æˆ· -> ç»Ÿä¸€USERåµŒå…¥ID
    3. [å¾®åšè¡¨æƒ…] -> å¾®åšè¡¨æƒ…åµŒå…¥ID
    4. Unicode Emoji (ğŸ˜‚ğŸŒ¿ç­‰) -> Unicodeè¡¨æƒ…åµŒå…¥ID
    5. å°ç±³å…³é”®è¯ -> å…³é”®è¯åµŒå…¥ID

    è¿”å›:
        special_ids: å‡ºç°çš„ç‰¹æ®Štoken IDåˆ—è¡¨ï¼ˆä¸é‡å¤ï¼‰
    """
    if not text or pd.isna(text):
        return []

    text = str(text)
    special_ids = set()

    # 1. æå–@ç”¨æˆ·
    at_pattern = re.compile(r'@([^\s@:ï¼š,ï¼Œã€‚ï¼!?ï¼Ÿ\[\]]+)')
    for match in at_pattern.finditer(text):
        username = match.group(1)
        if username in VIP_USER_TO_ID:
            special_ids.add(VIP_EMBED_OFFSET + VIP_USER_TO_ID[username])
        else:
            special_ids.add(USER_EMBED_ID)  # éVIPç”¨æˆ·

    # 2. æå–å¾®åšæ–¹æ‹¬å·è¡¨æƒ… [xxx]
    weibo_emoji_pattern = re.compile(r'\[([^\[\]]+)\]')
    for match in weibo_emoji_pattern.finditer(text):
        emoji_name = match.group(1)
        if emoji_name in WEIBO_EMOJI_TO_ID:
            special_ids.add(WEIBO_EMOJI_EMBED_OFFSET + WEIBO_EMOJI_TO_ID[emoji_name])

    # 3. æå–Unicode Emojiï¼ˆçœŸå®emojiå­—ç¬¦ï¼‰
    for emoji in UNICODE_EMOJI_LIST:
        if emoji in text:
            special_ids.add(UNICODE_EMOJI_EMBED_OFFSET + UNICODE_EMOJI_TO_ID[emoji])

    # 4. æå–å°ç±³å…³é”®è¯
    text_lower = text.lower()
    for keyword in XIAOMI_EMBED_KEYWORDS:
        if keyword.lower() in text_lower:
            special_ids.add(XIAOMI_EMBED_OFFSET + XIAOMI_KEYWORD_TO_ID[keyword])

    return list(special_ids)


# ==================== æ–‡æœ¬é¢„å¤„ç† ====================
def preprocess_text_for_bge(text, replace_users=True):
    """é¢„å¤„ç†æ–‡æœ¬ï¼Œç”¨äºBGEç¼–ç 

    å¤„ç†:
    1. @ç”¨æˆ· -> ä¿ç•™VIPç”¨æˆ·ï¼Œå…¶ä»–æ›¿æ¢ä¸º _USER_
    2. ä¿ç•™è¡¨æƒ…ã€ç‰¹æ®Šå­—ç¬¦ï¼ˆç”±BGEæ¨¡å‹å¤„ç†ï¼‰

    å‚æ•°:
        text: åŸå§‹æ–‡æœ¬
        replace_users: æ˜¯å¦æ›¿æ¢éVIPç”¨æˆ·

    è¿”å›:
        å¤„ç†åçš„æ–‡æœ¬
    """
    if not text or pd.isna(text):
        return ""

    text = str(text)

    if replace_users:
        # åŒ¹é… @ç”¨æˆ·å æ¨¡å¼
        def replace_user(match):
            username = match.group(1)
            if username in VIP_USER_TO_ID:
                return f"@{username}"  # ä¿ç•™VIPç”¨æˆ·
            return SPECIAL_TOKEN_USER  # æ›¿æ¢ä¸ºç‰¹æ®Štoken

        text = re.sub(r'@([^\s@]+)', replace_user, text)

    return text.strip()


# ==================== BERTæ¨¡å‹ç»„ä»¶ ====================
class BertEmbeddings(nn.Module):
    """BERT Embeddingså±‚"""
    def __init__(self, config):
        super().__init__()
        self.word_embeddings = nn.Embedding(config['vocab_size'], config['hidden_size'], padding_idx=0)
        self.position_embeddings = nn.Embedding(config['max_position_embeddings'], config['hidden_size'])
        self.token_type_embeddings = nn.Embedding(config['type_vocab_size'], config['hidden_size'])
        self.LayerNorm = nn.LayerNorm(config['hidden_size'], eps=config['layer_norm_eps'])
        self.dropout = nn.Dropout(config['hidden_dropout_prob'])

    def forward(self, input_ids, token_type_ids=None):
        seq_length = input_ids.size(1)
        position_ids = torch.arange(seq_length, dtype=torch.long, device=input_ids.device)
        position_ids = position_ids.unsqueeze(0).expand_as(input_ids)

        if token_type_ids is None:
            token_type_ids = torch.zeros_like(input_ids)

        embeddings = self.word_embeddings(input_ids)
        embeddings += self.position_embeddings(position_ids)
        embeddings += self.token_type_embeddings(token_type_ids)
        embeddings = self.LayerNorm(embeddings)
        embeddings = self.dropout(embeddings)
        return embeddings


class BertSelfAttention(nn.Module):
    """BERT Self-Attentionå±‚"""
    def __init__(self, config):
        super().__init__()
        self.num_attention_heads = config['num_attention_heads']
        self.attention_head_size = config['hidden_size'] // config['num_attention_heads']
        self.all_head_size = self.num_attention_heads * self.attention_head_size

        self.query = nn.Linear(config['hidden_size'], self.all_head_size)
        self.key = nn.Linear(config['hidden_size'], self.all_head_size)
        self.value = nn.Linear(config['hidden_size'], self.all_head_size)
        self.dropout = nn.Dropout(config['attention_probs_dropout_prob'])

    def transpose_for_scores(self, x):
        new_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)
        x = x.view(*new_shape)
        return x.permute(0, 2, 1, 3)

    def forward(self, hidden_states, attention_mask=None):
        query_layer = self.transpose_for_scores(self.query(hidden_states))
        key_layer = self.transpose_for_scores(self.key(hidden_states))
        value_layer = self.transpose_for_scores(self.value(hidden_states))

        attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))
        attention_scores = attention_scores / (self.attention_head_size ** 0.5)

        if attention_mask is not None:
            attention_scores = attention_scores + attention_mask

        attention_probs = F.softmax(attention_scores, dim=-1)
        attention_probs = self.dropout(attention_probs)

        context_layer = torch.matmul(attention_probs, value_layer)
        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()
        new_shape = context_layer.size()[:-2] + (self.all_head_size,)
        context_layer = context_layer.view(*new_shape)
        return context_layer


class BertLayer(nn.Module):
    """BERT Transformerå±‚"""
    def __init__(self, config):
        super().__init__()
        self.attention = BertSelfAttention(config)
        self.attention_output = nn.Linear(config['hidden_size'], config['hidden_size'])
        self.attention_norm = nn.LayerNorm(config['hidden_size'], eps=config['layer_norm_eps'])
        self.intermediate = nn.Linear(config['hidden_size'], config['intermediate_size'])
        self.output = nn.Linear(config['intermediate_size'], config['hidden_size'])
        self.output_norm = nn.LayerNorm(config['hidden_size'], eps=config['layer_norm_eps'])
        self.dropout = nn.Dropout(config['hidden_dropout_prob'])

    def forward(self, hidden_states, attention_mask=None):
        attention_output = self.attention(hidden_states, attention_mask)
        attention_output = self.dropout(self.attention_output(attention_output))
        hidden_states = self.attention_norm(hidden_states + attention_output)

        intermediate_output = F.gelu(self.intermediate(hidden_states))
        layer_output = self.dropout(self.output(intermediate_output))
        hidden_states = self.output_norm(hidden_states + layer_output)
        return hidden_states


class BertModel(nn.Module):
    """BERTæ¨¡å‹ï¼ˆä»…ç¼–ç å™¨ï¼‰"""
    def __init__(self, config):
        super().__init__()
        self.config = config
        self.embeddings = BertEmbeddings(config)
        self.layers = nn.ModuleList([BertLayer(config) for _ in range(config['num_hidden_layers'])])
        self.pooler = nn.Linear(config['hidden_size'], config['hidden_size'])

    def forward(self, input_ids, attention_mask=None, token_type_ids=None):
        if attention_mask is not None:
            extended_attention_mask = attention_mask.unsqueeze(1).unsqueeze(2)
            extended_attention_mask = (1.0 - extended_attention_mask) * -10000.0
        else:
            extended_attention_mask = None

        hidden_states = self.embeddings(input_ids, token_type_ids)

        for layer in self.layers:
            hidden_states = layer(hidden_states, extended_attention_mask)

        pooled_output = torch.tanh(self.pooler(hidden_states[:, 0]))
        return pooled_output, hidden_states


# ==================== ç‰¹æ®ŠTokenåµŒå…¥å±‚ ====================
class SpecialTokenEmbedding(nn.Module):
    """ç‰¹æ®ŠTokenå¯è®­ç»ƒåµŒå…¥å±‚

    ä¸ºVIPç”¨æˆ·ã€è¡¨æƒ…ç¬¦å·ã€å°ç±³å…³é”®è¯æä¾›ç‹¬ç«‹çš„å¯è®­ç»ƒåµŒå…¥ã€‚
    è¿™äº›åµŒå…¥ä¸BGEæ— å…³ï¼Œå¯ä»¥åœ¨BGEå†»ç»“æ—¶å•ç‹¬è®­ç»ƒã€‚
    """
    def __init__(self, num_embeddings=TOTAL_SPECIAL_EMBEDDINGS, embedding_dim=768, dropout=0.1):
        super().__init__()
        self.embedding = nn.Embedding(num_embeddings, embedding_dim)
        self.dropout = nn.Dropout(dropout)

        # Xavieråˆå§‹åŒ–
        nn.init.xavier_uniform_(self.embedding.weight)

    def forward(self, special_ids, special_mask):
        """
        å‚æ•°:
            special_ids: [batch, max_special_tokens] ç‰¹æ®Štoken ID
            special_mask: [batch, max_special_tokens] æœ‰æ•ˆä½ç½®æ©ç  (1=æœ‰æ•ˆ, 0=padding)

        è¿”å›:
            pooled: [batch, embedding_dim] æ± åŒ–åçš„ç‰¹æ®ŠtokenåµŒå…¥
        """
        # è·å–åµŒå…¥ [batch, max_special_tokens, embedding_dim]
        embeddings = self.embedding(special_ids)
        embeddings = self.dropout(embeddings)

        # æ©ç æ± åŒ–ï¼šå¯¹æœ‰æ•ˆtokenå–å¹³å‡
        mask_expanded = special_mask.unsqueeze(-1).float()  # [batch, max_special, 1]
        sum_embeddings = (embeddings * mask_expanded).sum(dim=1)  # [batch, dim]
        count = special_mask.sum(dim=1, keepdim=True).float().clamp(min=1)  # [batch, 1]
        pooled = sum_embeddings / count  # [batch, dim]

        return pooled


# ==================== æ³¨æ„åŠ›èåˆä¸é¢„æµ‹å¤´ ====================
class CrossAttentionFusion(nn.Module):
    """è·¨æ³¨æ„åŠ›èåˆå±‚

    å°†è¯„è®ºembeddingä½œä¸ºQueryï¼Œä¸Šä¸‹æ–‡ï¼ˆå¾®åš/æ ¹è¯„è®º/çˆ¶è¯„è®ºï¼‰ä½œä¸ºKey/Value
    """
    def __init__(self, hidden_size=768, num_heads=8, dropout=0.1):
        super().__init__()
        self.attention = nn.MultiheadAttention(
            hidden_size, num_heads, dropout=dropout, batch_first=True
        )
        self.norm = nn.LayerNorm(hidden_size)
        self.dropout = nn.Dropout(dropout)

    def forward(self, comment_emb, context_embs):
        """
        å‚æ•°:
            comment_emb: [batch, 768] è¯„è®ºembedding
            context_embs: [batch, 3, 768] ä¸Šä¸‹æ–‡embeddings (å¾®åš, æ ¹è¯„è®º, çˆ¶è¯„è®º)

        è¿”å›:
            fused: [batch, 768] èåˆåçš„embedding
        """
        # æ‰©å±•comment_embä¸º [batch, 1, 768] ä½œä¸ºQuery
        query = comment_emb.unsqueeze(1)

        # context_embsä½œä¸ºKeyå’ŒValue
        attn_output, _ = self.attention(query, context_embs, context_embs)

        # æ®‹å·®è¿æ¥
        fused = self.norm(comment_emb + self.dropout(attn_output.squeeze(1)))
        return fused


class DualPredictionHead(nn.Module):
    """åŒé¢„æµ‹å¤´ï¼šåŒæ—¶é¢„æµ‹å‡å€¼å’Œæ–¹å·®"""
    def __init__(self, input_size, hidden_size=256, dropout=0.1):
        super().__init__()
        self.shared = nn.Sequential(
            nn.Linear(input_size, hidden_size),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_size, hidden_size // 2),
            nn.ReLU(),
            nn.Dropout(dropout),
        )
        self.mu_head = nn.Linear(hidden_size // 2, 1)
        self.sigma_head = nn.Linear(hidden_size // 2, 1)

        # åˆå§‹åŒ–ï¼šè®©muåˆå§‹è¾“å‡ºæ¥è¿‘log(10)â‰ˆ2.3ï¼ˆå¯¹åº”å­è¯„è®ºæ•°ä¸º0ï¼‰
        # sigmaåˆå§‹è¾“å‡ºæ¥è¿‘1ï¼ˆåˆç†çš„ä¸ç¡®å®šæ€§ï¼‰
        nn.init.zeros_(self.mu_head.weight)
        nn.init.constant_(self.mu_head.bias, 2.3)  # log(10) â‰ˆ 2.3
        nn.init.zeros_(self.sigma_head.weight)
        nn.init.constant_(self.sigma_head.bias, 0.5)  # softplus(0.5) â‰ˆ 0.97

    def forward(self, x):
        """
        è¿”å›:
            mu: [batch] é¢„æµ‹å‡å€¼ï¼ˆlogç©ºé—´ï¼‰
            sigma: [batch] é¢„æµ‹æ ‡å‡†å·®ï¼ˆlogç©ºé—´ï¼Œé€šè¿‡softplusä¿è¯æ­£å€¼ï¼‰
        """
        shared = self.shared(x)
        mu = self.mu_head(shared).squeeze(-1)
        sigma = F.softplus(self.sigma_head(shared)).squeeze(-1) + 1e-4
        return mu, sigma


# ==================== Miniè½»é‡åŒ–æ¨¡å‹ ====================
class CommentPredictorMini(nn.Module):
    """è½»é‡åŒ–è¯„è®ºé¢„æµ‹ç¥ç»ç½‘ç»œ

    ç›¸æ¯”å®Œæ•´ç‰ˆCommentPredictorNNçš„ç®€åŒ–:
    1. ç§»é™¤Cross-Attentionï¼Œä½¿ç”¨ç®€å•çš„åŠ æƒå¹³å‡èåˆ
    2. æ›´å°çš„éšè—å±‚ç»´åº¦ï¼ˆ128 vs 256ï¼‰
    3. å¯é€‰åªä½¿ç”¨è¯„è®ºæ–‡æœ¬ï¼ˆä¸ä½¿ç”¨å¾®åš/æ ¹è¯„è®º/çˆ¶è¯„è®ºï¼‰
    4. æ›´å°‘çš„å‚æ•°é‡ï¼Œæ›´å¿«çš„è®­ç»ƒå’Œæ¨ç†é€Ÿåº¦

    é€‚ç”¨åœºæ™¯:
    - å¿«é€Ÿå®éªŒå’ŒåŸå‹éªŒè¯
    - èµ„æºå—é™ç¯å¢ƒ
    - ä½œä¸ºåŸºçº¿æ¨¡å‹å¯¹æ¯”
    """
    def __init__(self, bert_model, num_numeric_features, hidden_size=128, dropout=0.1,
                 freeze_bert=True, use_special_embeddings=True, use_context=True):
        super().__init__()
        self.bert = bert_model
        self.freeze_bert = freeze_bert
        self.use_special_embeddings = use_special_embeddings
        self.use_context = use_context  # æ˜¯å¦ä½¿ç”¨ä¸Šä¸‹æ–‡æ–‡æœ¬ï¼ˆå¾®åš/æ ¹è¯„è®º/çˆ¶è¯„è®ºï¼‰

        if freeze_bert:
            for param in self.bert.parameters():
                param.requires_grad = False

        # ç‰¹æ®ŠTokenåµŒå…¥å±‚ï¼ˆå¯é€‰ï¼Œç»´åº¦æ›´å°ï¼‰
        if use_special_embeddings:
            self.special_embedding = SpecialTokenEmbedding(
                num_embeddings=TOTAL_SPECIAL_EMBEDDINGS,
                embedding_dim=64,  # Miniç‰ˆä½¿ç”¨æ›´å°çš„ç»´åº¦
                dropout=dropout
            )
            special_dim = 64
        else:
            self.special_embedding = None
            special_dim = 0

        # æ–‡æœ¬åµŒå…¥ç»´åº¦
        if use_context:
            # ä½¿ç”¨å¯å­¦ä¹ çš„èåˆæƒé‡
            self.fusion_weights = nn.Parameter(torch.ones(4) / 4)
            text_dim = 768
        else:
            # åªä½¿ç”¨è¯„è®ºæ–‡æœ¬
            text_dim = 768

        # æ–‡æœ¬é™ç»´æŠ•å½±ï¼ˆ768 -> hidden_sizeï¼‰
        self.text_proj = nn.Sequential(
            nn.Linear(text_dim, hidden_size),
            nn.ReLU(),
            nn.Dropout(dropout),
        )

        # æ•°å€¼ç‰¹å¾æŠ•å½±
        self.numeric_proj = nn.Sequential(
            nn.Linear(num_numeric_features, 32),
            nn.ReLU(),
            nn.Dropout(dropout),
        )

        # é¢„æµ‹å¤´
        total_dim = hidden_size + 32 + special_dim
        self.prediction_head = nn.Sequential(
            nn.Linear(total_dim, hidden_size),
            nn.ReLU(),
            nn.Dropout(dropout),
        )

        # è¾“å‡ºå±‚
        self.mu_head = nn.Linear(hidden_size, 1)
        self.sigma_head = nn.Linear(hidden_size, 1)

        # åˆå§‹åŒ–è¾“å‡ºå±‚
        nn.init.zeros_(self.mu_head.weight)
        nn.init.constant_(self.mu_head.bias, 2.3)
        nn.init.zeros_(self.sigma_head.weight)
        nn.init.constant_(self.sigma_head.bias, 0.5)

    def encode_text(self, input_ids, attention_mask):
        """ç¼–ç å•ä¸ªæ–‡æœ¬ï¼Œè¿”å›[CLS] embedding"""
        pooled_output, _ = self.bert(input_ids, attention_mask)
        return pooled_output

    def forward(self, comment_ids, comment_mask, weibo_ids, weibo_mask,
                root_ids, root_mask, parent_ids, parent_mask, numeric_features,
                special_ids=None, special_mask=None):
        """
        å‚æ•°ä¸CommentPredictorNNç›¸åŒï¼Œä¿æŒæ¥å£ä¸€è‡´
        """
        # ç¼–ç è¯„è®ºæ–‡æœ¬
        comment_emb = self.encode_text(comment_ids, comment_mask)

        if self.use_context:
            # ç¼–ç ä¸Šä¸‹æ–‡æ–‡æœ¬
            weibo_emb = self.encode_text(weibo_ids, weibo_mask)
            root_emb = self.encode_text(root_ids, root_mask)
            parent_emb = self.encode_text(parent_ids, parent_mask)

            # åŠ æƒå¹³å‡èåˆï¼ˆå¯å­¦ä¹ æƒé‡ï¼‰
            weights = F.softmax(self.fusion_weights, dim=0)
            text_fused = (weights[0] * comment_emb +
                         weights[1] * weibo_emb +
                         weights[2] * root_emb +
                         weights[3] * parent_emb)
        else:
            # åªä½¿ç”¨è¯„è®ºæ–‡æœ¬
            text_fused = comment_emb

        # æ–‡æœ¬é™ç»´æŠ•å½±
        text_proj = self.text_proj(text_fused)

        # æ•°å€¼ç‰¹å¾æŠ•å½±
        numeric_proj = self.numeric_proj(numeric_features)

        # ç‰¹æ®ŠTokenåµŒå…¥
        if self.use_special_embeddings and special_ids is not None:
            special_emb = self.special_embedding(special_ids, special_mask)
            combined = torch.cat([text_proj, numeric_proj, special_emb], dim=1)
        else:
            combined = torch.cat([text_proj, numeric_proj], dim=1)

        # é¢„æµ‹
        hidden = self.prediction_head(combined)
        mu = self.mu_head(hidden).squeeze(-1)
        sigma = F.softplus(self.sigma_head(hidden)).squeeze(-1) + 1e-4

        return mu, sigma


class CommentPredictorNN(nn.Module):
    """è¯„è®ºé¢„æµ‹ç¥ç»ç½‘ç»œ

    ç»“æ„:
    1. BGEç¼–ç 4ä¸ªæ–‡æœ¬ï¼ˆè¯„è®º/å¾®åš/æ ¹è¯„è®º/çˆ¶è¯„è®ºï¼‰
    2. ç‰¹æ®ŠTokenåµŒå…¥ï¼ˆVIPç”¨æˆ·/è¡¨æƒ…/å…³é”®è¯ï¼Œå¯è®­ç»ƒï¼Œç‹¬ç«‹äºBGEï¼‰
    3. Cross-Attentionèåˆè¯„è®ºä¸ä¸Šä¸‹æ–‡
    4. æ‹¼æ¥æ•°å€¼ç‰¹å¾å’Œç‰¹æ®ŠåµŒå…¥
    5. åŒé¢„æµ‹å¤´è¾“å‡ºå‡å€¼å’Œæ–¹å·®
    """
    def __init__(self, bert_model, num_numeric_features, hidden_size=256, dropout=0.1,
                 freeze_bert=True, use_special_embeddings=True):
        super().__init__()
        self.bert = bert_model
        self.freeze_bert = freeze_bert
        self.use_special_embeddings = use_special_embeddings

        if freeze_bert:
            for param in self.bert.parameters():
                param.requires_grad = False

        # ç‰¹æ®ŠTokenåµŒå…¥å±‚ï¼ˆç‹¬ç«‹äºBGEï¼Œå§‹ç»ˆå¯è®­ç»ƒï¼‰
        if use_special_embeddings:
            self.special_embedding = SpecialTokenEmbedding(
                num_embeddings=TOTAL_SPECIAL_EMBEDDINGS,
                embedding_dim=128,  # è¾ƒå°çš„ç»´åº¦ï¼Œé¿å…è¿‡æ‹Ÿåˆ
                dropout=dropout
            )
            special_dim = 128
        else:
            self.special_embedding = None
            special_dim = 0

        # Cross-Attentionèåˆå±‚
        self.fusion = CrossAttentionFusion(hidden_size=768, num_heads=8, dropout=dropout)

        # æ•°å€¼ç‰¹å¾æŠ•å½±
        self.numeric_proj = nn.Sequential(
            nn.Linear(num_numeric_features, 64),
            nn.ReLU(),
            nn.Dropout(dropout),
        )

        # åŒé¢„æµ‹å¤´ï¼ˆè¾“å…¥ç»´åº¦ = 768 + 64 + special_dimï¼‰
        self.prediction_head = DualPredictionHead(
            input_size=768 + 64 + special_dim,
            hidden_size=hidden_size,
            dropout=dropout
        )

    def encode_text(self, input_ids, attention_mask):
        """ç¼–ç å•ä¸ªæ–‡æœ¬ï¼Œè¿”å›[CLS] embedding"""
        pooled_output, _ = self.bert(input_ids, attention_mask)
        return pooled_output

    def forward(self, comment_ids, comment_mask, weibo_ids, weibo_mask,
                root_ids, root_mask, parent_ids, parent_mask, numeric_features,
                special_ids=None, special_mask=None):
        """
        å‚æ•°:
            comment_ids, comment_mask: è¯„è®ºæ–‡æ¡ˆçš„tokenizedè¾“å…¥
            weibo_ids, weibo_mask: å¾®åšæ–‡æ¡ˆçš„tokenizedè¾“å…¥
            root_ids, root_mask: æ ¹è¯„è®ºæ–‡æ¡ˆçš„tokenizedè¾“å…¥
            parent_ids, parent_mask: çˆ¶è¯„è®ºæ–‡æ¡ˆçš„tokenizedè¾“å…¥
            numeric_features: [batch, num_numeric_features] æ•°å€¼ç‰¹å¾
            special_ids: [batch, max_special] ç‰¹æ®Štoken IDï¼ˆå¯é€‰ï¼‰
            special_mask: [batch, max_special] ç‰¹æ®Štokenæ©ç ï¼ˆå¯é€‰ï¼‰

        è¿”å›:
            mu: [batch] é¢„æµ‹å‡å€¼ï¼ˆlogç©ºé—´ï¼‰
            sigma: [batch] é¢„æµ‹æ ‡å‡†å·®ï¼ˆlogç©ºé—´ï¼‰
        """
        # ç¼–ç 4ä¸ªæ–‡æœ¬
        comment_emb = self.encode_text(comment_ids, comment_mask)
        weibo_emb = self.encode_text(weibo_ids, weibo_mask)
        root_emb = self.encode_text(root_ids, root_mask)
        parent_emb = self.encode_text(parent_ids, parent_mask)

        # å †å ä¸Šä¸‹æ–‡embedding: [batch, 3, 768]
        context_embs = torch.stack([weibo_emb, root_emb, parent_emb], dim=1)

        # Cross-Attentionèåˆ
        text_fused = self.fusion(comment_emb, context_embs)

        # æ•°å€¼ç‰¹å¾æŠ•å½±
        numeric_proj = self.numeric_proj(numeric_features)

        # ç‰¹æ®ŠTokenåµŒå…¥
        if self.use_special_embeddings and special_ids is not None:
            special_emb = self.special_embedding(special_ids, special_mask)
            # æ‹¼æ¥æ‰€æœ‰ç‰¹å¾
            combined = torch.cat([text_fused, numeric_proj, special_emb], dim=1)
        else:
            # æ‹¼æ¥æ–‡æœ¬å’Œæ•°å€¼ç‰¹å¾
            combined = torch.cat([text_fused, numeric_proj], dim=1)

        # åŒé¢„æµ‹å¤´
        mu, sigma = self.prediction_head(combined)

        return mu, sigma


# ==================== NLLæŸå¤±å‡½æ•° ====================
def nll_loss(y_true, mu, sigma):
    """å¯¹æ•°å°ºåº¦NLLæŸå¤±ï¼ˆæ•°å€¼ç¨³å®šç‰ˆï¼‰

    L = 0.5 * log(ÏƒÂ²) + (log(y+10) - Î¼)Â² / (2ÏƒÂ²)

    å‚æ•°:
        y_true: çœŸå®å€¼ï¼ˆåŸå§‹ç©ºé—´ï¼‰
        mu: é¢„æµ‹å‡å€¼ï¼ˆlogç©ºé—´ï¼‰
        sigma: é¢„æµ‹æ ‡å‡†å·®ï¼ˆlogç©ºé—´ï¼‰
    """
    # ç¡®ä¿y_trueéè´Ÿ
    y_true = torch.clamp(y_true, min=0)
    y_log = torch.log(y_true + LOG_OFFSET)

    # é™åˆ¶sigmaèŒƒå›´ï¼Œé¿å…æ•°å€¼é—®é¢˜
    sigma = torch.clamp(sigma, min=1e-4, max=100.0)

    # é™åˆ¶muèŒƒå›´ï¼Œé¿å…æç«¯å€¼
    mu = torch.clamp(mu, min=-10.0, max=20.0)

    # è®¡ç®—NLL
    nll = 0.5 * torch.log(sigma ** 2 + 1e-8) + ((y_log - mu) ** 2) / (2 * sigma ** 2 + 1e-8)

    return nll.mean()


# ==================== BGENNModelå°è£…ç±» ====================
class BGENNModel:
    """BGE + ç¥ç»ç½‘ç»œé¢„æµ‹æ¨¡å‹

    ä½¿ç”¨BGE-base-zh-v1.5ç¼–ç 4ä¸ªæ–‡æœ¬ï¼ˆè¯„è®º/å¾®åš/æ ¹è¯„è®º/çˆ¶è¯„è®ºï¼‰ï¼Œ
    é€šè¿‡Cross-Attentionèåˆï¼Œç»“åˆæ•°å€¼ç‰¹å¾ï¼ŒåŒé¢„æµ‹å¤´è¾“å‡ºå‡å€¼å’Œæ–¹å·®ã€‚
    æ”¯æŒå¯è®­ç»ƒçš„ç‰¹æ®ŠTokenåµŒå…¥ï¼ˆVIPç”¨æˆ·/è¡¨æƒ…/å…³é”®è¯ï¼‰ã€‚
    æ”¯æŒBF16æ··åˆç²¾åº¦è®­ç»ƒï¼ˆéœ€GPUæ”¯æŒï¼‰ã€‚
    """
    def __init__(self, freeze_bert=True, hidden_size=256, dropout=0.1,
                 use_special_embeddings=True, use_bf16=False, **kwargs):
        self.name = 'BGE_NN'
        self.freeze_bert = freeze_bert
        self.hidden_size = hidden_size
        self.dropout = dropout
        self.use_special_embeddings = use_special_embeddings
        self.use_bf16 = use_bf16  # BF16æ··åˆç²¾åº¦è®­ç»ƒï¼ˆé»˜è®¤å…³é—­ï¼‰
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.model = None
        self.tokenizer = None
        self.supports_uncertainty = True
        self.use_log_target = True

        # æ£€æŸ¥BF16æ”¯æŒ
        if self.use_bf16:
            if not torch.cuda.is_available():
                print("è­¦å‘Š: BF16éœ€è¦CUDAæ”¯æŒï¼Œå·²è‡ªåŠ¨ç¦ç”¨")
                self.use_bf16 = False
            elif not torch.cuda.is_bf16_supported():
                print("è­¦å‘Š: å½“å‰GPUä¸æ”¯æŒBF16ï¼Œå·²è‡ªåŠ¨ç¦ç”¨")
                self.use_bf16 = False

        # è®­ç»ƒå‚æ•°
        self.epochs = kwargs.get('epochs', 30)
        self.batch_size = kwargs.get('batch_size', 32)
        self.learning_rate = kwargs.get('learning_rate', 1e-4)
        self.patience = kwargs.get('patience', 5)

    def _load_bge_model(self):
        """åŠ è½½BGEæ¨¡å‹"""
        from tokenizers import Tokenizer

        model_path = str(BGE_MODEL_PATH)
        print(f"åŠ è½½BGEæ¨¡å‹: {model_path}")

        # åŠ è½½tokenizer
        self.tokenizer = Tokenizer.from_file(os.path.join(model_path, 'tokenizer.json'))
        self.tokenizer.enable_truncation(max_length=128)

        # åŠ è½½vocabè·å–pad_token_id
        with open(os.path.join(model_path, 'vocab.txt'), 'r', encoding='utf-8') as f:
            vocab = {line.strip(): idx for idx, line in enumerate(f)}
        self.pad_token_id = vocab.get('[PAD]', 0)

        # åŠ è½½é…ç½®
        with open(os.path.join(model_path, 'config.json'), 'r') as f:
            config = json.load(f)

        # åˆ›å»ºBERTæ¨¡å‹
        bert_model = BertModel(config)

        # åŠ è½½é¢„è®­ç»ƒæƒé‡
        state_dict = torch.load(
            os.path.join(model_path, 'pytorch_model.bin'),
            map_location='cpu'
        )

        # æ˜ å°„æƒé‡åç§°
        new_state_dict = {}
        for key, value in state_dict.items():
            new_key = key
            if key.startswith('bert.'):
                new_key = key[5:]
            if 'encoder.layer' in new_key:
                new_key = new_key.replace('encoder.layer', 'layers')
            if 'attention.self' in new_key:
                new_key = new_key.replace('attention.self', 'attention')
            if 'attention.output.dense' in new_key:
                new_key = new_key.replace('attention.output.dense', 'attention_output')
            if 'attention.output.LayerNorm' in new_key:
                new_key = new_key.replace('attention.output.LayerNorm', 'attention_norm')
            if 'intermediate.dense' in new_key:
                new_key = new_key.replace('intermediate.dense', 'intermediate')
            if 'output.dense' in new_key and 'attention' not in new_key:
                new_key = new_key.replace('output.dense', 'output')
            if 'output.LayerNorm' in new_key and 'attention' not in new_key:
                new_key = new_key.replace('output.LayerNorm', 'output_norm')
            if 'pooler.dense' in new_key:
                new_key = new_key.replace('pooler.dense', 'pooler')
            new_state_dict[new_key] = value

        # åŠ è½½æƒé‡
        missing, unexpected = bert_model.load_state_dict(new_state_dict, strict=False)
        print(f"BGEæƒé‡åŠ è½½å®Œæˆï¼ŒåŒ¹é…: {len(new_state_dict) - len(missing)}/{len(new_state_dict)}")

        return bert_model

    def fit(self, train_df, val_df, train_density=None, val_density=None, save_dir=None,
            test_df=None, test_density=None):
        """è®­ç»ƒæ¨¡å‹

        å‚æ•°:
            train_df: è®­ç»ƒæ•°æ®
            val_df: éªŒè¯æ•°æ®
            train_density: è®­ç»ƒé›†å¯†åº¦ç‰¹å¾
            val_density: éªŒè¯é›†å¯†åº¦ç‰¹å¾
            save_dir: æƒé‡ä¿å­˜ç›®å½•ï¼ˆå¦‚æœæä¾›ï¼Œæ¯ä¸ªepochåä¿å­˜bestå’Œlastæƒé‡ï¼‰
            test_df: æµ‹è¯•æ•°æ®ï¼ˆå¯é€‰ï¼Œæå‰åˆ†è¯ä»¥åŠ é€Ÿè¯„ä¼°ï¼‰
            test_density: æµ‹è¯•é›†å¯†åº¦ç‰¹å¾
        """
        from ..data.dataset import CommentDataset

        print(f"\nä½¿ç”¨è®¾å¤‡: {self.device}")
        print(f"å†»ç»“BGE: {self.freeze_bert}")

        # åŠ è½½BGEæ¨¡å‹
        bert_model = self._load_bge_model()

        # åˆ›å»ºæ•°æ®é›†ï¼ˆä¸€æ¬¡æ€§å®Œæˆæ‰€æœ‰åˆ†è¯ï¼‰
        print("åˆ›å»ºæ•°æ®é›†...")
        train_dataset = CommentDataset(train_df, self.tokenizer, train_density, max_length=128)
        val_dataset = CommentDataset(val_df, self.tokenizer, val_density, max_length=128)

        # å¦‚æœæä¾›äº†æµ‹è¯•é›†ï¼Œä¹Ÿä¸€å¹¶åˆ›å»ºï¼ˆé¿å…è¯„ä¼°æ—¶é‡æ–°åˆ†è¯ï¼‰
        if test_df is not None:
            print("åˆ›å»ºæµ‹è¯•æ•°æ®é›†ï¼ˆé¢„åˆ†è¯ï¼‰...")
            self._test_dataset = CommentDataset(test_df, self.tokenizer, test_density, max_length=128)
        else:
            self._test_dataset = None

        # ä¿å­˜è®­ç»ƒ/éªŒè¯æ•°æ®é›†ä¾›è¯„ä¼°ä½¿ç”¨
        self._train_dataset = train_dataset
        self._val_dataset = val_dataset

        # ä¼˜åŒ–çš„DataLoaderé…ç½®
        num_workers = min(8, os.cpu_count() or 4)
        train_loader = DataLoader(
            train_dataset,
            batch_size=self.batch_size,
            shuffle=True,
            num_workers=num_workers,
            pin_memory=True if self.device.type == 'cuda' else False,
            persistent_workers=True if num_workers > 0 else False
        )
        val_loader = DataLoader(
            val_dataset,
            batch_size=self.batch_size,
            shuffle=False,
            num_workers=num_workers,
            pin_memory=True if self.device.type == 'cuda' else False,
            persistent_workers=True if num_workers > 0 else False
        )

        # åˆ›å»ºæ¨¡å‹
        num_numeric_features = train_dataset.numeric_features.shape[1]
        self.model = CommentPredictorNN(
            bert_model,
            num_numeric_features,
            hidden_size=self.hidden_size,
            dropout=self.dropout,
            freeze_bert=self.freeze_bert,
            use_special_embeddings=self.use_special_embeddings
        ).to(self.device)

        # æ‰“å°æ¨¡å‹ä¿¡æ¯
        total_params = sum(p.numel() for p in self.model.parameters())
        trainable_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)
        print(f"æ¨¡å‹å‚æ•°: æ€»è®¡ {total_params:,}, å¯è®­ç»ƒ {trainable_params:,}")
        if self.use_special_embeddings:
            special_params = sum(p.numel() for p in self.model.special_embedding.parameters())
            print(f"  ç‰¹æ®ŠåµŒå…¥å‚æ•°: {special_params:,} (VIPç”¨æˆ·/è¡¨æƒ…/å…³é”®è¯)")

        # ä¼˜åŒ–å™¨
        optimizer = torch.optim.AdamW(
            filter(lambda p: p.requires_grad, self.model.parameters()),
            lr=self.learning_rate,
            weight_decay=0.01
        )

        # å­¦ä¹ ç‡è°ƒåº¦
        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
            optimizer, mode='min', factor=0.5, patience=2
        )

        # BF16æ··åˆç²¾åº¦è®­ç»ƒè®¾ç½®
        # æ³¨æ„: BF16ä¸éœ€è¦GradScalerï¼Œå› ä¸ºå…¶åŠ¨æ€èŒƒå›´è¶³å¤Ÿå¤§
        if self.use_bf16:
            print("å¯ç”¨BF16æ··åˆç²¾åº¦è®­ç»ƒ")
            autocast_ctx = torch.autocast(device_type='cuda', dtype=torch.bfloat16)
        else:
            # ä½¿ç”¨ç©ºä¸Šä¸‹æ–‡ç®¡ç†å™¨ï¼ˆä¸æ”¹å˜ç²¾åº¦ï¼‰
            from contextlib import nullcontext
            autocast_ctx = nullcontext()

        # è®­ç»ƒå¾ªç¯
        best_val_loss = float('inf')
        patience_counter = 0
        train_losses = []
        val_losses = []

        for epoch in range(self.epochs):
            # è®­ç»ƒ
            self.model.train()
            train_loss = 0
            nan_count = 0
            for batch in tqdm(train_loader, desc=f'Epoch {epoch+1}/{self.epochs}'):
                batch = {k: v.to(self.device) for k, v in batch.items()}

                optimizer.zero_grad()

                # BF16æ··åˆç²¾åº¦å‰å‘ä¼ æ’­
                with autocast_ctx:
                    mu, sigma = self.model(
                        batch['comment_ids'], batch['comment_mask'],
                        batch['weibo_ids'], batch['weibo_mask'],
                        batch['root_ids'], batch['root_mask'],
                        batch['parent_ids'], batch['parent_mask'],
                        batch['numeric_features'],
                        batch.get('special_ids'), batch.get('special_mask')
                    )
                    loss = nll_loss(batch['target'], mu, sigma)

                # NaNæ£€æµ‹
                if torch.isnan(loss) or torch.isinf(loss):
                    nan_count += 1
                    if nan_count <= 3:
                        print(f"\nè­¦å‘Š: æ£€æµ‹åˆ°NaN/InfæŸå¤±ï¼Œè·³è¿‡æ­¤æ‰¹æ¬¡")
                        print(f"  muèŒƒå›´: [{mu.min().item():.4f}, {mu.max().item():.4f}]")
                        print(f"  sigmaèŒƒå›´: [{sigma.min().item():.4f}, {sigma.max().item():.4f}]")
                        print(f"  targetèŒƒå›´: [{batch['target'].min().item():.4f}, {batch['target'].max().item():.4f}]")
                    continue

                loss.backward()

                torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)

                optimizer.step()
                train_loss += loss.item()

            if nan_count > 0:
                print(f"  æœ¬epochå…±æœ‰ {nan_count} ä¸ªæ‰¹æ¬¡å‡ºç°NaN/Infï¼Œå·²è·³è¿‡")

            train_loss /= max(len(train_loader) - nan_count, 1)
            train_losses.append(train_loss)

            # éªŒè¯
            self.model.eval()
            val_loss = 0
            with torch.no_grad():
                for batch in val_loader:
                    batch = {k: v.to(self.device) for k, v in batch.items()}

                    # BF16æ··åˆç²¾åº¦éªŒè¯
                    with autocast_ctx:
                        mu, sigma = self.model(
                            batch['comment_ids'], batch['comment_mask'],
                            batch['weibo_ids'], batch['weibo_mask'],
                            batch['root_ids'], batch['root_mask'],
                            batch['parent_ids'], batch['parent_mask'],
                            batch['numeric_features'],
                            batch.get('special_ids'), batch.get('special_mask')
                        )
                        loss = nll_loss(batch['target'], mu, sigma)

                    val_loss += loss.item()

            val_loss /= len(val_loader)
            val_losses.append(val_loss)

            print(f"Epoch {epoch+1}: train_loss={train_loss:.4f}, val_loss={val_loss:.4f}")

            scheduler.step(val_loss)

            # ä¿å­˜ last æƒé‡ï¼ˆæ¯ä¸ªepochéƒ½ä¿å­˜ï¼‰
            if save_dir is not None:
                last_path = Path(save_dir) / 'model_last.pt'
                torch.save({
                    'epoch': epoch + 1,
                    'model_state_dict': self.model.state_dict(),
                    'optimizer_state_dict': optimizer.state_dict(),
                    'scheduler_state_dict': scheduler.state_dict(),
                    'train_loss': train_loss,
                    'val_loss': val_loss,
                    'train_losses': train_losses,
                    'val_losses': val_losses,
                    'best_val_loss': best_val_loss,
                    'patience_counter': patience_counter,
                }, last_path)

            # Early stopping
            if val_loss < best_val_loss:
                best_val_loss = val_loss
                patience_counter = 0
                self.best_state = {k: v.cpu().clone() for k, v in self.model.state_dict().items()}

                # ä¿å­˜ best æƒé‡
                if save_dir is not None:
                    best_path = Path(save_dir) / 'model_best.pt'
                    torch.save({
                        'epoch': epoch + 1,
                        'model_state_dict': self.model.state_dict(),
                        'train_loss': train_loss,
                        'val_loss': val_loss,
                        'train_losses': train_losses,
                        'val_losses': val_losses,
                    }, best_path)
                    print(f"  ä¿å­˜æœ€ä½³æ¨¡å‹ (val_loss={val_loss:.4f})")
            else:
                patience_counter += 1
                if patience_counter >= self.patience:
                    print(f"Early stopping at epoch {epoch+1}")
                    break

        # æ¢å¤æœ€ä½³æ¨¡å‹
        if hasattr(self, 'best_state'):
            self.model.load_state_dict(self.best_state)
            self.model.to(self.device)

        self.train_losses = train_losses
        self.val_losses = val_losses

    def predict(self, df, density_df=None):
        """é¢„æµ‹ï¼ˆè¿”å›å‡å€¼ï¼‰"""
        mu, _ = self.predict_dist(df, density_df)
        return mu

    def predict_dist(self, df, density_df=None):
        """é¢„æµ‹åˆ†å¸ƒå‚æ•°"""
        from ..data.dataset import CommentDataset
        from contextlib import nullcontext

        dataset = CommentDataset(df, self.tokenizer, density_df, max_length=128)
        num_workers = min(4, os.cpu_count() or 2)
        loader = DataLoader(
            dataset,
            batch_size=self.batch_size,
            shuffle=False,
            num_workers=num_workers,
            pin_memory=True if self.device.type == 'cuda' else False
        )

        # BF16æ¨ç†ä¸Šä¸‹æ–‡
        if self.use_bf16:
            autocast_ctx = torch.autocast(device_type='cuda', dtype=torch.bfloat16)
        else:
            autocast_ctx = nullcontext()

        self.model.eval()
        all_mu = []
        all_sigma = []

        with torch.no_grad():
            for batch in loader:
                batch = {k: v.to(self.device) for k, v in batch.items()}

                with autocast_ctx:
                    mu, sigma = self.model(
                        batch['comment_ids'], batch['comment_mask'],
                        batch['weibo_ids'], batch['weibo_mask'],
                        batch['root_ids'], batch['root_mask'],
                        batch['parent_ids'], batch['parent_mask'],
                        batch['numeric_features'],
                        batch.get('special_ids'), batch.get('special_mask')
                    )

                # è½¬å›åŸå§‹ç©ºé—´ï¼Œç¡®ä¿éè´Ÿï¼ˆåœ¨FP32ä¸‹è¿›è¡Œï¼‰
                mu_orig = torch.exp(torch.clamp(mu.float(), max=20.0)) - LOG_OFFSET
                mu_orig = torch.clamp(mu_orig, min=0)  # ç¡®ä¿é¢„æµ‹å€¼éè´Ÿ
                all_mu.append(mu_orig.cpu().numpy())
                all_sigma.append(sigma.float().cpu().numpy())

        return np.concatenate(all_mu), np.concatenate(all_sigma)

    def evaluate_all(self, df=None, density_df=None, use_cached=None):
        """ä¸€æ¬¡æ€§è¯„ä¼°ï¼šè¿”å›é¢„æµ‹å‡å€¼ã€æ ‡å‡†å·®å’ŒNLLï¼ˆé¿å…é‡å¤åˆ›å»ºDatasetå’Œåˆ†è¯ï¼‰

        å‚æ•°:
            df: DataFrameæ•°æ®ï¼ˆå¦‚æœuse_cachedä¸ºNoneåˆ™å¿…é¡»æä¾›ï¼‰
            density_df: å¯†åº¦ç‰¹å¾ï¼ˆå¦‚æœuse_cachedä¸ºNoneåˆ™éœ€è¦æä¾›ï¼‰
            use_cached: ä½¿ç”¨ç¼“å­˜çš„æ•°æ®é›† ('train', 'val', 'test')ï¼Œå¦‚æœæä¾›åˆ™å¿½ç•¥dfå’Œdensity_df
        """
        from ..data.dataset import CommentDataset
        from contextlib import nullcontext

        # ä½¿ç”¨ç¼“å­˜çš„æ•°æ®é›†æˆ–åˆ›å»ºæ–°çš„
        if use_cached is not None:
            if use_cached == 'train' and hasattr(self, '_train_dataset'):
                dataset = self._train_dataset
            elif use_cached == 'val' and hasattr(self, '_val_dataset'):
                dataset = self._val_dataset
            elif use_cached == 'test' and hasattr(self, '_test_dataset') and self._test_dataset is not None:
                dataset = self._test_dataset
            else:
                raise ValueError(f"ç¼“å­˜æ•°æ®é›† '{use_cached}' ä¸å­˜åœ¨ï¼Œè¯·å…ˆè°ƒç”¨fitå¹¶ä¼ å…¥ç›¸åº”æ•°æ®")
        else:
            dataset = CommentDataset(df, self.tokenizer, density_df, max_length=128)

        # æ¨ç†æ—¶å¯ä»¥ä½¿ç”¨æ›´å¤§çš„æ‰¹æ¬¡
        eval_batch_size = self.batch_size * 2
        num_workers = min(4, os.cpu_count() or 2)
        loader = DataLoader(
            dataset,
            batch_size=eval_batch_size,
            shuffle=False,
            num_workers=num_workers,
            pin_memory=True if self.device.type == 'cuda' else False
        )

        if self.use_bf16:
            autocast_ctx = torch.autocast(device_type='cuda', dtype=torch.bfloat16)
        else:
            autocast_ctx = nullcontext()

        self.model.eval()
        all_mu = []
        all_sigma = []
        total_nll = 0
        count = 0

        with torch.no_grad():
            for batch in loader:
                batch = {k: v.to(self.device) for k, v in batch.items()}

                with autocast_ctx:
                    mu, sigma = self.model(
                        batch['comment_ids'], batch['comment_mask'],
                        batch['weibo_ids'], batch['weibo_mask'],
                        batch['root_ids'], batch['root_mask'],
                        batch['parent_ids'], batch['parent_mask'],
                        batch['numeric_features'],
                        batch.get('special_ids'), batch.get('special_mask')
                    )
                    loss = nll_loss(batch['target'], mu, sigma)

                total_nll += loss.item() * len(batch['target'])
                count += len(batch['target'])

                mu_orig = torch.exp(torch.clamp(mu.float(), max=20.0)) - LOG_OFFSET
                mu_orig = torch.clamp(mu_orig, min=0)
                all_mu.append(mu_orig.cpu().numpy())
                all_sigma.append(sigma.float().cpu().numpy())

        y_pred = np.concatenate(all_mu)
        y_std = np.concatenate(all_sigma)
        nll = total_nll / count

        return y_pred, y_std, nll

    def compute_nll(self, df, density_df=None):
        """è®¡ç®—NLLæŸå¤±"""
        from ..data.dataset import CommentDataset
        from contextlib import nullcontext

        dataset = CommentDataset(df, self.tokenizer, density_df, max_length=128)
        loader = DataLoader(dataset, batch_size=self.batch_size, shuffle=False)

        # BF16æ¨ç†ä¸Šä¸‹æ–‡
        if self.use_bf16:
            autocast_ctx = torch.autocast(device_type='cuda', dtype=torch.bfloat16)
        else:
            autocast_ctx = nullcontext()

        self.model.eval()
        total_nll = 0
        count = 0

        with torch.no_grad():
            for batch in loader:
                batch = {k: v.to(self.device) for k, v in batch.items()}

                with autocast_ctx:
                    mu, sigma = self.model(
                        batch['comment_ids'], batch['comment_mask'],
                        batch['weibo_ids'], batch['weibo_mask'],
                        batch['root_ids'], batch['root_mask'],
                        batch['parent_ids'], batch['parent_mask'],
                        batch['numeric_features'],
                        batch.get('special_ids'), batch.get('special_mask')
                    )
                    loss = nll_loss(batch['target'], mu, sigma)

                total_nll += loss.item() * len(batch['target'])
                count += len(batch['target'])

        return total_nll / count


# ==================== BGEMiniModelå°è£…ç±» ====================
class BGEMiniModel:
    """BGE + Miniè½»é‡åŒ–ç¥ç»ç½‘ç»œé¢„æµ‹æ¨¡å‹

    ç›¸æ¯”BGENNModelçš„ç®€åŒ–:
    1. ä½¿ç”¨åŠ æƒå¹³å‡èåˆä»£æ›¿Cross-Attentionï¼ˆå‡å°‘çº¦60%å‚æ•°ï¼‰
    2. æ›´å°çš„éšè—å±‚ç»´åº¦ï¼ˆ128 vs 256ï¼‰
    3. å¯é€‰åªä½¿ç”¨è¯„è®ºæ–‡æœ¬ï¼ˆuse_context=Falseæ—¶æ¨ç†é€Ÿåº¦æå‡4å€ï¼‰
    4. ç‰¹æ®ŠåµŒå…¥ç»´åº¦æ›´å°ï¼ˆ64 vs 128ï¼‰

    é€‚ç”¨åœºæ™¯:
    - å¿«é€Ÿå®éªŒå’ŒåŸå‹éªŒè¯
    - èµ„æºå—é™ç¯å¢ƒï¼ˆæ˜¾å­˜ä¸è¶³ï¼‰
    - ä½œä¸ºåŸºçº¿æ¨¡å‹å¯¹æ¯”
    """
    def __init__(self, freeze_bert=True, hidden_size=128, dropout=0.1,
                 use_special_embeddings=True, use_context=True, use_bf16=False, **kwargs):
        self.name = 'BGE_Mini'
        self.freeze_bert = freeze_bert
        self.hidden_size = hidden_size
        self.dropout = dropout
        self.use_special_embeddings = use_special_embeddings
        self.use_context = use_context
        self.use_bf16 = use_bf16
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.model = None
        self.tokenizer = None
        self.supports_uncertainty = True
        self.use_log_target = True

        # æ£€æŸ¥BF16æ”¯æŒ
        if self.use_bf16:
            if not torch.cuda.is_available():
                print("è­¦å‘Š: BF16éœ€è¦CUDAæ”¯æŒï¼Œå·²è‡ªåŠ¨ç¦ç”¨")
                self.use_bf16 = False
            elif not torch.cuda.is_bf16_supported():
                print("è­¦å‘Š: å½“å‰GPUä¸æ”¯æŒBF16ï¼Œå·²è‡ªåŠ¨ç¦ç”¨")
                self.use_bf16 = False

        # è®­ç»ƒå‚æ•°ï¼ˆMiniç‰ˆé»˜è®¤æ›´å¤šepochï¼Œæ›´å¤§å­¦ä¹ ç‡ï¼‰
        self.epochs = kwargs.get('epochs', 50)
        self.batch_size = kwargs.get('batch_size', 64)
        self.learning_rate = kwargs.get('learning_rate', 2e-4)
        self.patience = kwargs.get('patience', 7)

    def _load_bge_model(self):
        """åŠ è½½BGEæ¨¡å‹"""
        from tokenizers import Tokenizer

        model_path = str(BGE_MODEL_PATH)
        print(f"åŠ è½½BGEæ¨¡å‹: {model_path}")

        # åŠ è½½tokenizer
        self.tokenizer = Tokenizer.from_file(os.path.join(model_path, 'tokenizer.json'))
        self.tokenizer.enable_truncation(max_length=128)

        # åŠ è½½vocabè·å–pad_token_id
        with open(os.path.join(model_path, 'vocab.txt'), 'r', encoding='utf-8') as f:
            vocab = {line.strip(): idx for idx, line in enumerate(f)}
        self.pad_token_id = vocab.get('[PAD]', 0)

        # åŠ è½½é…ç½®
        with open(os.path.join(model_path, 'config.json'), 'r') as f:
            config = json.load(f)

        # åˆ›å»ºBERTæ¨¡å‹
        bert_model = BertModel(config)

        # åŠ è½½é¢„è®­ç»ƒæƒé‡
        state_dict = torch.load(
            os.path.join(model_path, 'pytorch_model.bin'),
            map_location='cpu'
        )

        # æ˜ å°„æƒé‡åç§°
        new_state_dict = {}
        for key, value in state_dict.items():
            new_key = key
            if key.startswith('bert.'):
                new_key = key[5:]
            if 'encoder.layer' in new_key:
                new_key = new_key.replace('encoder.layer', 'layers')
            if 'attention.self' in new_key:
                new_key = new_key.replace('attention.self', 'attention')
            if 'attention.output.dense' in new_key:
                new_key = new_key.replace('attention.output.dense', 'attention_output')
            if 'attention.output.LayerNorm' in new_key:
                new_key = new_key.replace('attention.output.LayerNorm', 'attention_norm')
            if 'intermediate.dense' in new_key:
                new_key = new_key.replace('intermediate.dense', 'intermediate')
            if 'output.dense' in new_key and 'attention' not in new_key:
                new_key = new_key.replace('output.dense', 'output')
            if 'output.LayerNorm' in new_key and 'attention' not in new_key:
                new_key = new_key.replace('output.LayerNorm', 'output_norm')
            if 'pooler.dense' in new_key:
                new_key = new_key.replace('pooler.dense', 'pooler')
            new_state_dict[new_key] = value

        # åŠ è½½æƒé‡
        missing, unexpected = bert_model.load_state_dict(new_state_dict, strict=False)
        print(f"BGEæƒé‡åŠ è½½å®Œæˆï¼ŒåŒ¹é…: {len(new_state_dict) - len(missing)}/{len(new_state_dict)}")

        return bert_model

    def fit(self, train_df, val_df, train_density=None, val_density=None, save_dir=None,
            test_df=None, test_density=None):
        """è®­ç»ƒæ¨¡å‹

        å‚æ•°:
            train_df: è®­ç»ƒæ•°æ®
            val_df: éªŒè¯æ•°æ®
            train_density: è®­ç»ƒé›†å¯†åº¦ç‰¹å¾
            val_density: éªŒè¯é›†å¯†åº¦ç‰¹å¾
            save_dir: æƒé‡ä¿å­˜ç›®å½•ï¼ˆå¦‚æœæä¾›ï¼Œæ¯ä¸ªepochåä¿å­˜bestå’Œlastæƒé‡ï¼‰
            test_df: æµ‹è¯•æ•°æ®ï¼ˆå¯é€‰ï¼Œæå‰åˆ†è¯ä»¥åŠ é€Ÿè¯„ä¼°ï¼‰
            test_density: æµ‹è¯•é›†å¯†åº¦ç‰¹å¾
        """
        from ..data.dataset import CommentDataset
        from contextlib import nullcontext

        print(f"\n[Miniæ¨¡å‹] ä½¿ç”¨è®¾å¤‡: {self.device}")
        print(f"  å†»ç»“BGE: {self.freeze_bert}")
        print(f"  ä½¿ç”¨ä¸Šä¸‹æ–‡: {self.use_context}")
        print(f"  ç‰¹æ®ŠåµŒå…¥: {self.use_special_embeddings}")

        # åŠ è½½BGEæ¨¡å‹
        bert_model = self._load_bge_model()

        # åˆ›å»ºæ•°æ®é›†ï¼ˆä¸€æ¬¡æ€§å®Œæˆæ‰€æœ‰åˆ†è¯ï¼‰
        print("åˆ›å»ºæ•°æ®é›†...")
        train_dataset = CommentDataset(train_df, self.tokenizer, train_density, max_length=128)
        val_dataset = CommentDataset(val_df, self.tokenizer, val_density, max_length=128)

        # å¦‚æœæä¾›äº†æµ‹è¯•é›†ï¼Œä¹Ÿä¸€å¹¶åˆ›å»ºï¼ˆé¿å…è¯„ä¼°æ—¶é‡æ–°åˆ†è¯ï¼‰
        if test_df is not None:
            print("åˆ›å»ºæµ‹è¯•æ•°æ®é›†ï¼ˆé¢„åˆ†è¯ï¼‰...")
            self._test_dataset = CommentDataset(test_df, self.tokenizer, test_density, max_length=128)
        else:
            self._test_dataset = None

        # ä¿å­˜è®­ç»ƒ/éªŒè¯æ•°æ®é›†ä¾›è¯„ä¼°ä½¿ç”¨
        self._train_dataset = train_dataset
        self._val_dataset = val_dataset

        # DataLoaderé…ç½®
        num_workers = min(8, os.cpu_count() or 4)
        train_loader = DataLoader(
            train_dataset,
            batch_size=self.batch_size,
            shuffle=True,
            num_workers=num_workers,
            pin_memory=True if self.device.type == 'cuda' else False,
            persistent_workers=True if num_workers > 0 else False
        )
        val_loader = DataLoader(
            val_dataset,
            batch_size=self.batch_size,
            shuffle=False,
            num_workers=num_workers,
            pin_memory=True if self.device.type == 'cuda' else False,
            persistent_workers=True if num_workers > 0 else False
        )

        # åˆ›å»ºMiniæ¨¡å‹
        num_numeric_features = train_dataset.numeric_features.shape[1]
        self.model = CommentPredictorMini(
            bert_model,
            num_numeric_features,
            hidden_size=self.hidden_size,
            dropout=self.dropout,
            freeze_bert=self.freeze_bert,
            use_special_embeddings=self.use_special_embeddings,
            use_context=self.use_context
        ).to(self.device)

        # æ‰“å°æ¨¡å‹ä¿¡æ¯
        total_params = sum(p.numel() for p in self.model.parameters())
        trainable_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)
        print(f"æ¨¡å‹å‚æ•°: æ€»è®¡ {total_params:,}, å¯è®­ç»ƒ {trainable_params:,}")
        if self.use_special_embeddings:
            special_params = sum(p.numel() for p in self.model.special_embedding.parameters())
            print(f"  ç‰¹æ®ŠåµŒå…¥å‚æ•°: {special_params:,}")

        # ä¼˜åŒ–å™¨
        optimizer = torch.optim.AdamW(
            filter(lambda p: p.requires_grad, self.model.parameters()),
            lr=self.learning_rate,
            weight_decay=0.01
        )

        # å­¦ä¹ ç‡è°ƒåº¦
        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
            optimizer, mode='min', factor=0.5, patience=3
        )

        # BF16æ··åˆç²¾åº¦
        if self.use_bf16:
            print("å¯ç”¨BF16æ··åˆç²¾åº¦è®­ç»ƒ")
            autocast_ctx = torch.autocast(device_type='cuda', dtype=torch.bfloat16)
        else:
            autocast_ctx = nullcontext()

        # è®­ç»ƒå¾ªç¯
        best_val_loss = float('inf')
        patience_counter = 0
        train_losses = []
        val_losses = []

        for epoch in range(self.epochs):
            # è®­ç»ƒ
            self.model.train()
            train_loss = 0
            nan_count = 0
            for batch in tqdm(train_loader, desc=f'Epoch {epoch+1}/{self.epochs}'):
                batch = {k: v.to(self.device) for k, v in batch.items()}

                optimizer.zero_grad()

                with autocast_ctx:
                    mu, sigma = self.model(
                        batch['comment_ids'], batch['comment_mask'],
                        batch['weibo_ids'], batch['weibo_mask'],
                        batch['root_ids'], batch['root_mask'],
                        batch['parent_ids'], batch['parent_mask'],
                        batch['numeric_features'],
                        batch.get('special_ids'), batch.get('special_mask')
                    )
                    loss = nll_loss(batch['target'], mu, sigma)

                if torch.isnan(loss) or torch.isinf(loss):
                    nan_count += 1
                    continue

                loss.backward()
                torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)
                optimizer.step()
                train_loss += loss.item()

            if nan_count > 0:
                print(f"  æœ¬epochå…±æœ‰ {nan_count} ä¸ªæ‰¹æ¬¡å‡ºç°NaN/Infï¼Œå·²è·³è¿‡")

            train_loss /= max(len(train_loader) - nan_count, 1)
            train_losses.append(train_loss)

            # éªŒè¯
            self.model.eval()
            val_loss = 0
            with torch.no_grad():
                for batch in val_loader:
                    batch = {k: v.to(self.device) for k, v in batch.items()}

                    with autocast_ctx:
                        mu, sigma = self.model(
                            batch['comment_ids'], batch['comment_mask'],
                            batch['weibo_ids'], batch['weibo_mask'],
                            batch['root_ids'], batch['root_mask'],
                            batch['parent_ids'], batch['parent_mask'],
                            batch['numeric_features'],
                            batch.get('special_ids'), batch.get('special_mask')
                        )
                        loss = nll_loss(batch['target'], mu, sigma)

                    val_loss += loss.item()

            val_loss /= len(val_loader)
            val_losses.append(val_loss)

            print(f"Epoch {epoch+1}: train_loss={train_loss:.4f}, val_loss={val_loss:.4f}")

            scheduler.step(val_loss)

            # ä¿å­˜ last æƒé‡ï¼ˆæ¯ä¸ªepochéƒ½ä¿å­˜ï¼‰
            if save_dir is not None:
                last_path = Path(save_dir) / 'model_last.pt'
                torch.save({
                    'epoch': epoch + 1,
                    'model_state_dict': self.model.state_dict(),
                    'optimizer_state_dict': optimizer.state_dict(),
                    'scheduler_state_dict': scheduler.state_dict(),
                    'train_loss': train_loss,
                    'val_loss': val_loss,
                    'train_losses': train_losses,
                    'val_losses': val_losses,
                    'best_val_loss': best_val_loss,
                    'patience_counter': patience_counter,
                }, last_path)

            # Early stopping
            if val_loss < best_val_loss:
                best_val_loss = val_loss
                patience_counter = 0
                self.best_state = {k: v.cpu().clone() for k, v in self.model.state_dict().items()}

                # ä¿å­˜ best æƒé‡
                if save_dir is not None:
                    best_path = Path(save_dir) / 'model_best.pt'
                    torch.save({
                        'epoch': epoch + 1,
                        'model_state_dict': self.model.state_dict(),
                        'train_loss': train_loss,
                        'val_loss': val_loss,
                        'train_losses': train_losses,
                        'val_losses': val_losses,
                    }, best_path)
                    print(f"  ä¿å­˜æœ€ä½³æ¨¡å‹ (val_loss={val_loss:.4f})")
            else:
                patience_counter += 1
                if patience_counter >= self.patience:
                    print(f"Early stopping at epoch {epoch+1}")
                    break

        # æ¢å¤æœ€ä½³æ¨¡å‹
        if hasattr(self, 'best_state'):
            self.model.load_state_dict(self.best_state)
            self.model.to(self.device)

        self.train_losses = train_losses
        self.val_losses = val_losses

    def predict(self, df, density_df=None):
        """é¢„æµ‹ï¼ˆè¿”å›å‡å€¼ï¼‰"""
        mu, _ = self.predict_dist(df, density_df)
        return mu

    def predict_dist(self, df, density_df=None):
        """é¢„æµ‹åˆ†å¸ƒå‚æ•°"""
        from ..data.dataset import CommentDataset
        from contextlib import nullcontext

        dataset = CommentDataset(df, self.tokenizer, density_df, max_length=128)
        num_workers = min(4, os.cpu_count() or 2)
        loader = DataLoader(
            dataset,
            batch_size=self.batch_size,
            shuffle=False,
            num_workers=num_workers,
            pin_memory=True if self.device.type == 'cuda' else False
        )

        if self.use_bf16:
            autocast_ctx = torch.autocast(device_type='cuda', dtype=torch.bfloat16)
        else:
            autocast_ctx = nullcontext()

        self.model.eval()
        all_mu = []
        all_sigma = []

        with torch.no_grad():
            for batch in loader:
                batch = {k: v.to(self.device) for k, v in batch.items()}

                with autocast_ctx:
                    mu, sigma = self.model(
                        batch['comment_ids'], batch['comment_mask'],
                        batch['weibo_ids'], batch['weibo_mask'],
                        batch['root_ids'], batch['root_mask'],
                        batch['parent_ids'], batch['parent_mask'],
                        batch['numeric_features'],
                        batch.get('special_ids'), batch.get('special_mask')
                    )

                mu_orig = torch.exp(torch.clamp(mu.float(), max=20.0)) - LOG_OFFSET
                mu_orig = torch.clamp(mu_orig, min=0)
                all_mu.append(mu_orig.cpu().numpy())
                all_sigma.append(sigma.float().cpu().numpy())

        return np.concatenate(all_mu), np.concatenate(all_sigma)

    def evaluate_all(self, df=None, density_df=None, use_cached=None):
        """ä¸€æ¬¡æ€§è¯„ä¼°ï¼šè¿”å›é¢„æµ‹å‡å€¼ã€æ ‡å‡†å·®å’ŒNLLï¼ˆé¿å…é‡å¤åˆ›å»ºDatasetå’Œåˆ†è¯ï¼‰

        å‚æ•°:
            df: DataFrameæ•°æ®ï¼ˆå¦‚æœuse_cachedä¸ºNoneåˆ™å¿…é¡»æä¾›ï¼‰
            density_df: å¯†åº¦ç‰¹å¾ï¼ˆå¦‚æœuse_cachedä¸ºNoneåˆ™éœ€è¦æä¾›ï¼‰
            use_cached: ä½¿ç”¨ç¼“å­˜çš„æ•°æ®é›† ('train', 'val', 'test')ï¼Œå¦‚æœæä¾›åˆ™å¿½ç•¥dfå’Œdensity_df
        """
        from ..data.dataset import CommentDataset
        from contextlib import nullcontext

        # ä½¿ç”¨ç¼“å­˜çš„æ•°æ®é›†æˆ–åˆ›å»ºæ–°çš„
        if use_cached is not None:
            if use_cached == 'train' and hasattr(self, '_train_dataset'):
                dataset = self._train_dataset
            elif use_cached == 'val' and hasattr(self, '_val_dataset'):
                dataset = self._val_dataset
            elif use_cached == 'test' and hasattr(self, '_test_dataset') and self._test_dataset is not None:
                dataset = self._test_dataset
            else:
                raise ValueError(f"ç¼“å­˜æ•°æ®é›† '{use_cached}' ä¸å­˜åœ¨ï¼Œè¯·å…ˆè°ƒç”¨fitå¹¶ä¼ å…¥ç›¸åº”æ•°æ®")
        else:
            dataset = CommentDataset(df, self.tokenizer, density_df, max_length=128)

        # æ¨ç†æ—¶å¯ä»¥ä½¿ç”¨æ›´å¤§çš„æ‰¹æ¬¡
        eval_batch_size = self.batch_size * 2
        num_workers = min(4, os.cpu_count() or 2)
        loader = DataLoader(
            dataset,
            batch_size=eval_batch_size,
            shuffle=False,
            num_workers=num_workers,
            pin_memory=True if self.device.type == 'cuda' else False
        )

        if self.use_bf16:
            autocast_ctx = torch.autocast(device_type='cuda', dtype=torch.bfloat16)
        else:
            autocast_ctx = nullcontext()

        self.model.eval()
        all_mu = []
        all_sigma = []
        total_nll = 0
        count = 0

        with torch.no_grad():
            for batch in loader:
                batch = {k: v.to(self.device) for k, v in batch.items()}

                with autocast_ctx:
                    mu, sigma = self.model(
                        batch['comment_ids'], batch['comment_mask'],
                        batch['weibo_ids'], batch['weibo_mask'],
                        batch['root_ids'], batch['root_mask'],
                        batch['parent_ids'], batch['parent_mask'],
                        batch['numeric_features'],
                        batch.get('special_ids'), batch.get('special_mask')
                    )
                    loss = nll_loss(batch['target'], mu, sigma)

                total_nll += loss.item() * len(batch['target'])
                count += len(batch['target'])

                mu_orig = torch.exp(torch.clamp(mu.float(), max=20.0)) - LOG_OFFSET
                mu_orig = torch.clamp(mu_orig, min=0)
                all_mu.append(mu_orig.cpu().numpy())
                all_sigma.append(sigma.float().cpu().numpy())

        y_pred = np.concatenate(all_mu)
        y_std = np.concatenate(all_sigma)
        nll = total_nll / count

        return y_pred, y_std, nll

    def compute_nll(self, df, density_df=None):
        """è®¡ç®—NLLæŸå¤±"""
        from ..data.dataset import CommentDataset
        from contextlib import nullcontext

        dataset = CommentDataset(df, self.tokenizer, density_df, max_length=128)
        loader = DataLoader(dataset, batch_size=self.batch_size, shuffle=False)

        if self.use_bf16:
            autocast_ctx = torch.autocast(device_type='cuda', dtype=torch.bfloat16)
        else:
            autocast_ctx = nullcontext()

        self.model.eval()
        total_nll = 0
        count = 0

        with torch.no_grad():
            for batch in loader:
                batch = {k: v.to(self.device) for k, v in batch.items()}

                with autocast_ctx:
                    mu, sigma = self.model(
                        batch['comment_ids'], batch['comment_mask'],
                        batch['weibo_ids'], batch['weibo_mask'],
                        batch['root_ids'], batch['root_mask'],
                        batch['parent_ids'], batch['parent_mask'],
                        batch['numeric_features'],
                        batch.get('special_ids'), batch.get('special_mask')
                    )
                    loss = nll_loss(batch['target'], mu, sigma)

                total_nll += loss.item() * len(batch['target'])
                count += len(batch['target'])

        return total_nll / count
